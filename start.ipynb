{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel & MobileODT Cervical Cancer Screening\n",
    "\n",
    "I am working on this project for a Kaggle competition:\n",
    "\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\n",
    "\n",
    "My goal is accurately identifie a woman’s cervix type (type 1, type 2 or type 3) based on the image. As a rtaning set I am using about 1500 images (about 500 images for every type of cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR1 = 'train/Type_1'\n",
    "TRAIN_DIR2 = 'train/Type_2'\n",
    "TRAIN_DIR3 = 'train/Type_3'\n",
    "TEST_DIR = 'test'\n",
    "IMG_SIZE = 50 # The photos have different size. I will resize it 50 by 50 just to start (I think, we need to use a better resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pixel_depth = 255.0 \n",
    "def create_train_data(TRAIN_DIR, label):\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if not img[0].isdigit():\n",
    "            continue\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = (cv2.imread(path,cv2.IMREAD_GRAYSCALE) - pixel_depth / 2) / pixel_depth\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:07<00:00,  4.21it/s]\n",
      "100%|██████████| 772/772 [03:42<00:00,  4.04it/s]\n",
      "100%|██████████| 441/441 [02:06<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "create_train_data(TRAIN_DIR1, [1.0,0.0,0.0])\n",
    "create_train_data(TRAIN_DIR2, [0.0,1.0,0.0])\n",
    "create_train_data(TRAIN_DIR3, [0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(training_data)\n",
    "np.save('train_data_50_50_gray.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_50_50_gray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data[:-500]\n",
    "test = train_data[-500:-250]\n",
    "valid = train_data[-250: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "valid_x = np.array([i[0] for i in valid]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "valid_y = np.array([i[1] for i in valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = test_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "valid_x = valid_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "X = X.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 50\n",
    "num_labels = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "    #tf_result_dataset = tf.constant(test_pictures)\n",
    "\n",
    "    num_hidden_nodes = 1554\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    logits = tf.matmul(layer_1, weights2) + biases2\n",
    "    \n",
    "    betta = 0.01\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, \n",
    "        logits=logits) + betta *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2)\n",
    "                                 + tf.nn.l2_loss(biases2)))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    #result = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_result_dataset, weights1)+biases1),weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (951, 2500) (951, 3)\n",
      "Validation set (250, 2500) (250, 3)\n",
      "Test set (250, 2500) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X.shape, Y.shape)\n",
    "print('Validation set', valid_x.shape, valid_y.shape)\n",
    "print('Test set', test_x.shape, test_y.shape)\n",
    "#print('Result set', test_pictures.shape, test_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-aaae494659d2>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 15223.503906\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 32.0%\n",
      "Minibatch loss at step 500: 100.955147\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 1000: 1.728694\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 51.2%\n",
      "Minibatch loss at step 1500: 0.781542\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 44.8%\n",
      "Minibatch loss at step 2000: 0.968269\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 44.4%\n",
      "Test accuracy: 45.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that validation accuracy decreases after step 1000 and that test accuracy and validation accuracy are almost equal. Thus, I am goint to try to do 1001 steps only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-f966112a1c79>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 15268.683594\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 50.8%\n",
      "Minibatch loss at step 500: 100.725555\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 40.0%\n",
      "Minibatch loss at step 1000: 1.378992\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 50.0%\n",
      "Test accuracy: 47.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47.6 is not super good, but is is better than 33% that we can get if we will predit always the same type.\n",
    "\n",
    "Now, I am going to try increase the picture size up to 100 by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [01:19<00:00,  3.93it/s]\n",
      "100%|██████████| 782/782 [03:54<00:00,  4.36it/s]\n",
      "100%|██████████| 451/451 [01:50<00:00,  4.63it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 100\n",
    "\n",
    "training_data = []\n",
    "create_train_data(TRAIN_DIR1, [1.0,0.0,0.0])\n",
    "create_train_data(TRAIN_DIR2, [0.0,1.0,0.0])\n",
    "create_train_data(TRAIN_DIR3, [0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(training_data)\n",
    "np.save('train_data_100_100_gray.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_100_100_gray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data[:-500]\n",
    "test = train_data[-500:-250]\n",
    "valid = train_data[-250: ]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "valid_x = np.array([i[0] for i in valid]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "valid_y = np.array([i[1] for i in valid])\n",
    "\n",
    "test_x = test_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "valid_x = valid_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "X = X.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 100\n",
    "num_labels = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "\n",
    "    num_hidden_nodes = 1554\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    logits = tf.matmul(layer_1, weights2) + biases2\n",
    "    \n",
    "    betta = 0.01\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, \n",
    "        logits=logits) + betta *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2)\n",
    "                                 + tf.nn.l2_loss(biases2)))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1)+biases1),weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (981, 10000) (981, 3)\n",
      "Validation set (250, 10000) (250, 3)\n",
      "Test set (250, 10000) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X.shape, Y.shape)\n",
    "print('Validation set', valid_x.shape, valid_y.shape)\n",
    "print('Test set', test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-13f45ba3b1e9>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 60470.289062\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 55.2%\n",
      "Minibatch loss at step 500: 414.891235\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 1000: 4.122334\n",
      "Minibatch accuracy: 57.0%\n",
      "Validation accuracy: 48.8%\n",
      "Minibatch loss at step 1500: 1.744074\n",
      "Minibatch accuracy: 61.7%\n",
      "Validation accuracy: 42.0%\n",
      "Minibatch loss at step 2000: 1.140596\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 41.6%\n",
      "Minibatch loss at step 2500: 0.922524\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 52.0%\n",
      "Minibatch loss at step 3000: 0.827986\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 51.2%\n",
      "Test accuracy: 45.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The better resolution did not give me the better result.\n",
    "\n",
    "\n",
    "I have used the photos in grayscale. I think, the colors can be important for this problem since it looks like pictures of Type 1 have more red aread than of Type 2 and 3. Thus, I am going to try COLOR_BGR2RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixel_depth = 255.0 \n",
    "def create_train_data(TRAIN_DIR, label):\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if not img[0].isdigit():\n",
    "            continue\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = (cv2.imread(path,cv2.COLOR_BGR2RGB) - pixel_depth / 2) / pixel_depth\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [03:24<00:00,  1.60it/s]\n",
      "100%|██████████| 782/782 [09:45<00:00,  1.59it/s]\n",
      "100%|██████████| 451/451 [04:47<00:00,  1.86it/s]\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 50\n",
    "\n",
    "training_data = []\n",
    "create_train_data(TRAIN_DIR1, [1.0,0.0,0.0])\n",
    "create_train_data(TRAIN_DIR2, [0.0,1.0,0.0])\n",
    "create_train_data(TRAIN_DIR3, [0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(training_data)\n",
    "np.save('train_data_50_50_color.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_50_50_color.npy')\n",
    "train = train_data[:-500]\n",
    "test = train_data[-500:-250]\n",
    "valid = train_data[-250: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981, 2)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "valid_x = np.array([i[0] for i in valid]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "valid_y = np.array([i[1] for i in valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(981, 50, 50, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = test_x.reshape((-1, IMG_SIZE * IMG_SIZE * 3)).astype(np.float32)\n",
    "valid_x = valid_x.reshape((-1, IMG_SIZE * IMG_SIZE * 3)).astype(np.float32)\n",
    "X = X.reshape((-1, IMG_SIZE * IMG_SIZE * 3)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (981, 7500) (981, 3)\n",
      "Validation set (250, 7500) (250, 3)\n",
      "Test set (250, 7500) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X.shape, Y.shape)\n",
    "print('Validation set', valid_x.shape, valid_y.shape)\n",
    "print('Test set', test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 50\n",
    "num_labels = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size * 3))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "    \n",
    "    num_hidden_nodes = 1554\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size * 3, num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    logits = tf.matmul(layer_1, weights2) + biases2\n",
    "    \n",
    "    betta = 0.01\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, \n",
    "        logits=logits) + betta *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2)\n",
    "                                 + tf.nn.l2_loss(biases2)))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1)+biases1),weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-50-13f45ba3b1e9>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 45411.992188\n",
      "Minibatch accuracy: 35.9%\n",
      "Validation accuracy: 53.2%\n",
      "Minibatch loss at step 500: 303.543274\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 49.2%\n",
      "Minibatch loss at step 1000: 3.095729\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 48.4%\n",
      "Minibatch loss at step 1500: 1.043535\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 47.6%\n",
      "Minibatch loss at step 2000: 0.873229\n",
      "Minibatch accuracy: 77.3%\n",
      "Validation accuracy: 42.8%\n",
      "Minibatch loss at step 2500: 0.995510\n",
      "Minibatch accuracy: 68.0%\n",
      "Validation accuracy: 38.8%\n",
      "Minibatch loss at step 3000: 0.863765\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 40.8%\n",
      "Test accuracy: 39.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that I had the highest accuracy after the first step. Thus I will try to run the same algoritm with less steps: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-51-ab2e1dba1191>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 45459.871094\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 17.2%\n",
      "Minibatch loss at step 20: 37474.875000\n",
      "Minibatch accuracy: 47.7%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 40: 30587.208984\n",
      "Minibatch accuracy: 69.5%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 60: 25029.021484\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 50.4%\n",
      "Minibatch loss at step 80: 20482.097656\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 46.8%\n",
      "Minibatch loss at step 100: 16760.386719\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 48.8%\n",
      "Minibatch loss at step 120: 13715.205078\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 44.8%\n",
      "Minibatch loss at step 140: 11223.781250\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 43.6%\n",
      "Minibatch loss at step 160: 9184.925781\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 180: 7516.029297\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 200: 6150.630859\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 220: 5033.438477\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 240: 4118.894531\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 46.4%\n",
      "Minibatch loss at step 260: 3370.587646\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 48.8%\n",
      "Minibatch loss at step 280: 2758.407715\n",
      "Minibatch accuracy: 89.1%\n",
      "Validation accuracy: 45.2%\n",
      "Minibatch loss at step 300: 2257.365234\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 46.0%\n",
      "Minibatch loss at step 320: 1847.298950\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 45.6%\n",
      "Minibatch loss at step 340: 1511.751099\n",
      "Minibatch accuracy: 90.6%\n",
      "Validation accuracy: 46.8%\n",
      "Minibatch loss at step 360: 1237.201416\n",
      "Minibatch accuracy: 92.2%\n",
      "Validation accuracy: 48.8%\n",
      "Minibatch loss at step 380: 1012.542603\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 49.2%\n",
      "Minibatch loss at step 400: 828.715332\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 45.2%\n",
      "Minibatch loss at step 420: 678.233337\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 50.8%\n",
      "Minibatch loss at step 440: 555.196106\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 49.2%\n",
      "Minibatch loss at step 460: 454.354431\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 48.0%\n",
      "Minibatch loss at step 480: 372.020508\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 50.0%\n",
      "Test accuracy: 49.2%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 500\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 20 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Great! Using the full color pictures works better than grayscale. Now I am going to increase the number of pictures in the training set (some days ago additional pictures became avalible for this competiotion.)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
