{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel & MobileODT Cervical Cancer Screening\n",
    "\n",
    "I am working on this project for a Kaggle competition:\n",
    "\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\n",
    "\n",
    "My goal is accurately identifie a woman’s cervix type (type 1, type 2 or type 3) based on the image. As a rtaning set I am using about 1500 images (about 500 images for every type of cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR1 = 'train/Type_1'\n",
    "TRAIN_DIR2 = 'train/Type_2'\n",
    "TRAIN_DIR3 = 'train/Type_3'\n",
    "TEST_DIR = 'test'\n",
    "IMG_SIZE = 50 # The photos have different size. I will resize it 50 by 50 just to start (I think, we need to use a better resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pixel_depth = 255.0 \n",
    "def create_train_data(TRAIN_DIR, label):\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if not img[0].isdigit():\n",
    "            continue\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = (cv2.imread(path,cv2.IMREAD_GRAYSCALE) - pixel_depth / 2) / pixel_depth\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 241/241 [01:07<00:00,  4.21it/s]\n",
      "100%|██████████| 772/772 [03:42<00:00,  4.04it/s]\n",
      "100%|██████████| 441/441 [02:06<00:00,  4.07it/s]\n"
     ]
    }
   ],
   "source": [
    "create_train_data(TRAIN_DIR1, [1.0,0.0,0.0])\n",
    "create_train_data(TRAIN_DIR2, [0.0,1.0,0.0])\n",
    "create_train_data(TRAIN_DIR3, [0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1451"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(training_data)\n",
    "np.save('train_data_50_50_gray.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_50_50_gray.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train_data[:-500]\n",
    "test = train_data[-500:-250]\n",
    "valid = train_data[-250: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "valid_x = np.array([i[0] for i in valid]).reshape(-1,IMG_SIZE,IMG_SIZE,1)\n",
    "valid_y = np.array([i[1] for i in valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = test_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "valid_x = valid_x.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)\n",
    "X = X.reshape((-1, IMG_SIZE * IMG_SIZE)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "image_size = 50\n",
    "num_labels = 3\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_x)\n",
    "    tf_test_dataset = tf.constant(test_x)\n",
    "    #tf_result_dataset = tf.constant(test_pictures)\n",
    "\n",
    "    num_hidden_nodes = 1554\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, num_hidden_nodes]))\n",
    "    weights2 = tf.Variable(\n",
    "        tf.truncated_normal([num_hidden_nodes, num_labels]))\n",
    "    biases1 = tf.Variable(tf.zeros([num_hidden_nodes]))\n",
    "    biases2 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "    layer_1 = tf.nn.relu(tf.matmul(tf_train_dataset,weights1)+biases1)\n",
    "    logits = tf.matmul(layer_1, weights2) + biases2\n",
    "    \n",
    "    betta = 0.01\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, \n",
    "        logits=logits) + betta *(tf.nn.l2_loss(weights1) + tf.nn.l2_loss(biases1) + tf.nn.l2_loss(weights2)\n",
    "                                 + tf.nn.l2_loss(biases2)))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights1)+biases1),weights2) + biases2)\n",
    "    #result = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_result_dataset, weights1)+biases1),weights2) + biases2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (951, 2500) (951, 3)\n",
      "Validation set (250, 2500) (250, 3)\n",
      "Test set (250, 2500) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X.shape, Y.shape)\n",
    "print('Validation set', valid_x.shape, valid_y.shape)\n",
    "print('Test set', test_x.shape, test_y.shape)\n",
    "#print('Result set', test_pictures.shape, test_names.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-aaae494659d2>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 15223.503906\n",
      "Minibatch accuracy: 46.9%\n",
      "Validation accuracy: 32.0%\n",
      "Minibatch loss at step 500: 100.955147\n",
      "Minibatch accuracy: 95.3%\n",
      "Validation accuracy: 50.0%\n",
      "Minibatch loss at step 1000: 1.728694\n",
      "Minibatch accuracy: 60.9%\n",
      "Validation accuracy: 51.2%\n",
      "Minibatch loss at step 1500: 0.781542\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 44.8%\n",
      "Minibatch loss at step 2000: 0.968269\n",
      "Minibatch accuracy: 75.8%\n",
      "Validation accuracy: 44.4%\n",
      "Test accuracy: 45.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see that validation accuracy decreases after step 1000 and that test accuracy and validation accuracy are almost equal. Thus, I am goint to try to do 1001 steps only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-14-f966112a1c79>:4: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Two layers netowrk, Initialized\n",
      "Minibatch loss at step 0: 15268.683594\n",
      "Minibatch accuracy: 34.4%\n",
      "Validation accuracy: 50.8%\n",
      "Minibatch loss at step 500: 100.725555\n",
      "Minibatch accuracy: 96.1%\n",
      "Validation accuracy: 40.0%\n",
      "Minibatch loss at step 1000: 1.378992\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 50.0%\n",
      "Test accuracy: 47.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "\ttf.initialize_all_variables().run()\n",
    "\tprint(\"Two layers netowrk, Initialized\")\n",
    "\tfor step in range(num_steps):\n",
    "\t\toffset = (step * batch_size) % (Y.shape[0] - batch_size)\n",
    "\t\tbatch_data = X[offset:(offset + batch_size), :]\n",
    "\t\tbatch_labels = Y[offset:(offset + batch_size), :]\n",
    "\t\tfeed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "\t\t_, l, predictions = session.run(\n",
    "\t\t\t[optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\t\tif (step % 500 == 0):\n",
    "\t\t\tprint(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "\t\t\tprint(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "\t\t\tprint(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "\t\t\t\tvalid_prediction.eval(), valid_y))\n",
    "\tprint(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "47.6 is not super good, but is is better than 33% that we can get if we will predit always the same type.\n",
    "\n",
    "Now, I am going to try increase the picture size up to 100 by 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
