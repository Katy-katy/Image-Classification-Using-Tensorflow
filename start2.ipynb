{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intel & MobileODT Cervical Cancer Screening\n",
    "\n",
    "I am working on this project for a Kaggle competition:\n",
    "\n",
    "https://www.kaggle.com/c/intel-mobileodt-cervical-cancer-screening\n",
    "\n",
    "My goal is accurately identifie a woman’s cervix type (type 1, type 2 or type 3) based on the image. As a rtaning set I am using about 1500 images (about 500 images for every type of cancer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2                 # working with, mainly resizing, images\n",
    "import numpy as np         # dealing with arrays\n",
    "import os                  # dealing with directories\n",
    "from random import shuffle # mixing up or currently ordered data that might lead our network astray in training.\n",
    "from tqdm import tqdm      # a nice pretty percentage bar for tasks. Thanks to viewer Daniel BA1/4hler for this suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR1 = 'train/Type_1'\n",
    "TRAIN_DIR2 = 'train/Type_2'\n",
    "TRAIN_DIR3 = 'train/Type_3'\n",
    "TEST_DIR = 'test'\n",
    "IMG_SIZE = 50\n",
    "LR = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixel_depth = 255.0 \n",
    "def create_train_data(TRAIN_DIR, label):\n",
    "    for img in tqdm(os.listdir(TRAIN_DIR)):\n",
    "        if not img[0].isdigit():\n",
    "            continue\n",
    "        path = os.path.join(TRAIN_DIR,img)\n",
    "        img = (cv2.imread(path,cv2.COLOR_BGR2RGB) - pixel_depth / 2) / pixel_depth\n",
    "        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n",
    "        training_data.append([np.array(img),np.array(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 251/251 [03:37<00:00,  1.39it/s]\n",
      "100%|██████████| 782/782 [10:59<00:00,  1.22it/s]\n",
      "100%|██████████| 451/451 [05:49<00:00,  1.55it/s]\n"
     ]
    }
   ],
   "source": [
    "create_train_data(TRAIN_DIR1, [1.0,0.0,0.0])\n",
    "create_train_data(TRAIN_DIR2, [0.0,1.0,0.0])\n",
    "create_train_data(TRAIN_DIR3, [0.0,0.0,1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 251 pictures of Type 1, 782 pictures of Type 2, and 451 of Type 3. Thus, if we always will predict Type 2, we will have accuracy about 53%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shuffle(training_data)\n",
    "np.save('train_data_50_50_color.npy', training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_50_50_color.npy')\n",
    "\n",
    "train = train_data[:-500]\n",
    "test = train_data[-250: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\n",
    "test_y = np.array([i[1] for i in test])\n",
    "\n",
    "test_x = test_x.reshape((-1, IMG_SIZE * IMG_SIZE * 3)).astype(np.float32)\n",
    "X = X.reshape((-1, IMG_SIZE * IMG_SIZE * 3)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (981, 7500) (981, 3)\n",
      "Test set (250, 7500) (250, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Training set', X.shape, Y.shape)\n",
    "print('Test set', test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_nodes_hl1 = 1500\n",
    "n_nodes_hl2 = 1500\n",
    "n_nodes_hl3 = 1500\n",
    "\n",
    "n_classes = 3\n",
    "batch_size = 100\n",
    "hm_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float')\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "hidden_1_layer = {'f_fum':n_nodes_hl1,\n",
    "                  'weight':tf.Variable(tf.random_normal([len(X[0]), n_nodes_hl1])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "\n",
    "hidden_2_layer = {'f_fum':n_nodes_hl2,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "\n",
    "hidden_3_layer = {'f_fum':n_nodes_hl3,\n",
    "                  'weight':tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])),\n",
    "                  'bias':tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "\n",
    "output_layer = {'f_fum':None,\n",
    "                'weight':tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])),\n",
    "                'bias':tf.Variable(tf.random_normal([n_classes])),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_model(data):\n",
    "\n",
    "    l1 = tf.add(tf.matmul(data,hidden_1_layer['weight']), hidden_1_layer['bias'])\n",
    "    l1 = tf.nn.relu(l1)\n",
    "\n",
    "    l2 = tf.add(tf.matmul(l1,hidden_2_layer['weight']), hidden_2_layer['bias'])\n",
    "    l2 = tf.nn.relu(l2)\n",
    "\n",
    "    l3 = tf.add(tf.matmul(l2,hidden_3_layer['weight']), hidden_3_layer['bias'])\n",
    "    l3 = tf.nn.relu(l3)\n",
    "\n",
    "    output = tf.matmul(l3,output_layer['weight']) + output_layer['bias']\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "\tprediction = neural_network_model(x)\n",
    "\tcost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y) )\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.initialize_all_variables())\n",
    "\t    \n",
    "\t\tfor epoch in range(hm_epochs):\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\ti=0\n",
    "\t\t\twhile i < len(X):\n",
    "\t\t\t\tstart = i\n",
    "\t\t\t\tend = i+batch_size\n",
    "\t\t\t\tbatch_x = np.array(X[start:end])\n",
    "\t\t\t\tbatch_y = np.array(Y[start:end])\n",
    "\n",
    "\t\t\t\t_, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "\t\t\t\t                                              y: batch_y})\n",
    "\t\t\t\tepoch_loss += c\n",
    "\t\t\t\ti+=batch_size\n",
    "\t\t\t\t\n",
    "\t\t\tif (epoch % 10 == 0): print('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\t\tcorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\t\taccuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\n",
    "\t\tprint('Accuracy:',accuracy.eval({x:test_x, y:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-cb7b046c42e7>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 200 loss: 2472224.32812\n",
      "Epoch 11 completed out of 200 loss: 372179.392822\n",
      "Epoch 21 completed out of 200 loss: 0.0\n",
      "Epoch 31 completed out of 200 loss: 0.0\n",
      "Epoch 41 completed out of 200 loss: 0.0\n",
      "Epoch 51 completed out of 200 loss: 0.0\n",
      "Epoch 61 completed out of 200 loss: 0.0\n",
      "Epoch 71 completed out of 200 loss: 0.0\n",
      "Epoch 81 completed out of 200 loss: 0.0\n",
      "Epoch 91 completed out of 200 loss: 0.0\n",
      "Epoch 101 completed out of 200 loss: 0.0\n",
      "Epoch 111 completed out of 200 loss: 0.0\n",
      "Epoch 121 completed out of 200 loss: 0.0\n",
      "Epoch 131 completed out of 200 loss: 0.0\n",
      "Epoch 141 completed out of 200 loss: 0.0\n",
      "Epoch 151 completed out of 200 loss: 0.0\n",
      "Epoch 161 completed out of 200 loss: 0.0\n",
      "Epoch 171 completed out of 200 loss: 0.0\n",
      "Epoch 181 completed out of 200 loss: 0.0\n",
      "Epoch 191 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.46\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train_neural_network(x):\n",
    "\tprediction = neural_network_model(x)\n",
    "\tcost = tf.reduce_mean( tf.nn.softmax_cross_entropy_with_logits(logits = prediction,labels = y) )\n",
    "\toptimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\twith tf.Session() as sess:\n",
    "\t\tsess.run(tf.initialize_all_variables())\n",
    "\t    \n",
    "\t\tfor epoch in range(hm_epochs):\n",
    "\t\t\tepoch_loss = 0\n",
    "\t\t\ti=0\n",
    "\t\t\twhile i < len(X):\n",
    "\t\t\t\tstart = i\n",
    "\t\t\t\tend = i+batch_size\n",
    "\t\t\t\tbatch_x = np.array(X[start:end])\n",
    "\t\t\t\tbatch_y = np.array(Y[start:end])\n",
    "\n",
    "\t\t\t\t_, c = sess.run([optimizer, cost], feed_dict={x: batch_x,\n",
    "\t\t\t\t                                              y: batch_y})\n",
    "\t\t\t\tepoch_loss += c\n",
    "\t\t\t\ti+=batch_size\n",
    "\t\t\t\t\n",
    "\t\t\tif (epoch % 10 == 0): \n",
    "\t\t\t\tprint('Epoch', epoch+1, 'completed out of',hm_epochs,'loss:',epoch_loss)\n",
    "\t\t\t\tcorrect = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "\t\t\t\taccuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "\t\t\t\tprint('Accuracy:',accuracy.eval({x:test_x, y:test_y}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-13-656e44f9c1c9>:7: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch 1 completed out of 200 loss: 2681276.04688\n",
      "Accuracy: 0.352\n",
      "Epoch 11 completed out of 200 loss: 5826.27813721\n",
      "Accuracy: 0.436\n",
      "Epoch 21 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 31 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 41 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 51 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 61 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 71 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 81 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 91 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 101 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 111 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 121 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 131 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 141 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 151 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 161 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 171 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 181 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n",
      "Epoch 191 completed out of 200 loss: 0.0\n",
      "Accuracy: 0.428\n"
     ]
    }
   ],
   "source": [
    "train_neural_network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tflearn\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.estimator import regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load('train_data_50_50_gray.npy')\n",
    "\n",
    "train = train_data[:-500]\n",
    "test = train_data[-500: ]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1).astype(np.float32)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1).astype(np.float32)\n",
    "test_y = np.array([i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 3, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'types1_3-{}-{}.model'.format(LR, '2conv-basic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.00446\u001b[0m\u001b[0m | time: 3.546s\n",
      "| Adam | epoch: 003 | loss: 1.00446 - acc: 0.5399 -- iter: 896/951\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m0.99751\u001b[0m\u001b[0m | time: 4.807s\n",
      "| Adam | epoch: 003 | loss: 0.99751 - acc: 0.5464 | val_loss: 1.00470 - val_acc: 0.5260 -- iter: 951/951\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "model.fit({'input': X}, {'targets': Y}, n_epoch=3, validation_set=({'input': test_x}, {'targets': test_y}), \n",
    "    snapshot_step=500, show_metric=True, run_id=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c81f602a8444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_data_100_100_gray.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = 100 # I am going to try increase the image size\n",
    "\n",
    "training_data = []\n",
    "train_data = np.load('train_data_100_100_gray.npy')\n",
    "\n",
    "train = train_data[:-500]\n",
    "test = train_data[-500: ]\n",
    "\n",
    "X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,1).astype(np.float32)\n",
    "Y = np.array([i[1] for i in train])\n",
    "\n",
    "test_x = np.array([i[0] for i in test]).reshape(-1,IMG_SIZE,IMG_SIZE,1).astype(np.float32)\n",
    "test_y = np.array([i[1] for i in test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "convnet = input_data(shape=[None, IMG_SIZE, IMG_SIZE, 1], name='input')\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 128, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 64, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = conv_2d(convnet, 32, 5, activation='relu')\n",
    "convnet = max_pool_2d(convnet, 5)\n",
    "\n",
    "convnet = fully_connected(convnet, 1024, activation='relu')\n",
    "convnet = dropout(convnet, 0.8)\n",
    "\n",
    "convnet = fully_connected(convnet, 3, activation='softmax')\n",
    "convnet = regression(convnet, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
    "\n",
    "model = tflearn.DNN(convnet, tensorboard_dir='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'types1_3-{}-{}.model'.format(LR, '2conv-basic_100_100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
